# -*- coding: utf-8 -*-
"""Secondary_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kNDYkyR6PxFy4wuioeAuuInK9OEOYEXR
"""

# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# SECONDARY ANALYSIS TOOLS
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
"""
secondary_analysis.py

Script used to perform ad-hoc and secondary analyses on the
Final MeSH Network analysis.

This script requires access to the following files:
scripts/python/config.py
data/processed/FILE_PREFIX_final_network_with_relevance.json
data/processed/FILE_PREFIX_contextual_relevance.db

This script performs:
1. Node Relevancy Analysis:
   - Retrieves articles and relevancy scores for a specific node (MeSH term).
2. Edge Relevancy Analysis:
   - Retrieves articles and relevancy scores for a specific relationship (Edge).
3. Network-Wide Analysis:
   - Identifies top-scoring articles across the entire refined network.
4. Data Export:
   - Converts JSON network files into structured Excel files (Nodes & Edges sheets).

This saves finalized reports in:
results/
"""

import os
import sys
import json
import sqlite3
import pandas as pd

# <<< 1. CONFIGURATION SETUP >>>
# Finding the config.py file
if "__file__" in globals():
    current_dir = os.path.dirname(os.path.abspath(__file__))
else:
    current_dir = os.path.abspath(os.getcwd())

if current_dir not in sys.path:
    sys.path.append(current_dir)

import config

# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# CORE FUNCTIONS
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

def analyze_node_relevancy(node_name, limit=20):
    """
    Pulls relevancy scores for articles associated with a specific node.
    """
    db_path = config.FILES['relevance_db']

    if not os.path.exists(db_path):
        print(f"ERROR: Database not found at {db_path}")
        return

    print(f"\n<<< Searching for Node: '{node_name}' >>>")
    conn = sqlite3.connect(db_path)

    query = f"""
        SELECT pm_id, title, year, score_betweenness_centrality, contributing_seeds
        FROM article_relevance_scores
        WHERE contributing_seeds LIKE ?
        ORDER BY score_betweenness_centrality DESC
    """

    try:
        df = pd.read_sql_query(query, conn, params=(f'%{node_name}%',))
        print(f" -> Found {len(df)} articles related to '{node_name}'.")

        if not df.empty:
            print(f"Top {limit} results:")
            print(df.head(limit).to_string(index=False))

            # Optional: Save to CSV
            out_file = os.path.join(config.RESULTS_DIR, f"{config.FILE_PREFIX}_relevance_{node_name}.csv")
            df.to_csv(out_file, index=False)
            print(f"Saved full results to: {out_file}")

    except Exception as e:
        print(f"Error querying database: {e}")
    finally:
        conn.close()


def analyze_edge_relevancy(node1, node2, limit=20):
    """
    Pulls relevancy scores for articles associated with a specific EDGE (relationship).
    """
    db_path = config.FILES['relevance_db']

    if not os.path.exists(db_path):
        print(f"ERROR: Database not found at {db_path}")
        return

    print(f"\n<<< Searching for Edge: '{node1}' <--> '{node2}' >>>")
    conn = sqlite3.connect(db_path)

    query = f"""
        SELECT pm_id, title, year, score_betweenness_centrality, contributing_seeds
        FROM article_relevance_scores
        WHERE contributing_seeds LIKE ? AND contributing_seeds LIKE ?
        ORDER BY score_betweenness_centrality DESC
    """

    try:
        df = pd.read_sql_query(query, conn, params=(f'%{node1}%', f'%{node2}%'))
        print(f" -> Found {len(df)} articles linking '{node1}' and '{node2}'.")

        if not df.empty:
            print(f"Top {limit} results:")
            print(df.head(limit).to_string(index=False))

            # Optional: Save to CSV
            out_name = f"{config.FILE_PREFIX}_edge_relevance_{node1}_{node2}.csv"
            # Clean filename of potentially bad characters
            out_name = out_name.replace(" ", "_").replace(",", "")
            out_file = os.path.join(config.RESULTS_DIR, out_name)

            df.to_csv(out_file, index=False)
            print(f"Saved full results to: {out_file}")

    except Exception as e:
        print(f"Error querying database: {e}")
    finally:
        conn.close()


def get_top_network_articles(limit=20):
    """
    Retrieves the highest scoring articles across the entire refined network.
    These are often major reviews or seminal papers.
    """
    db_path = config.FILES['relevance_db']

    if not os.path.exists(db_path):
        print(f"ERROR: Database not found at {db_path}")
        return

    print(f"\n<<< Top {limit} Network-Wide Articles >>>")
    conn = sqlite3.connect(db_path)

    query = "SELECT pm_id, title, year, score_betweenness_centrality FROM article_relevance_scores ORDER BY score_betweenness_centrality DESC"

    try:
        df = pd.read_sql_query(query, conn)
        print(df.head(limit).to_string(index=False))
    except Exception as e:
        print(f"Error: {e}")
    finally:
        conn.close()


def convert_network_json_to_excel(target_file_key='final_network'):
    """
    Converts a JSON network file to an Excel file, saved in the RESULTS folder.
    """
    if target_file_key not in config.FILES:
        print(f"ERROR: Key '{target_file_key}' not found in config.FILES.")
        return

    input_path = config.FILES[target_file_key]

    # Force the output to live in the results folder
    filename = os.path.basename(input_path).replace('.json', '_export.xlsx')
    output_path = os.path.join(config.RESULTS_DIR, filename)

    print(f"\n<<< Converting JSON to Excel >>>")
    print(f"Input:  {input_path}")
    print(f"Output: {output_path}")

    if not os.path.exists(input_path):
        print("ERROR: Input file does not exist. Run the Master Script first.")
        return

    try:
        with open(input_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        nodes = data.get('elements', {}).get('nodes', [])
        edges = data.get('elements', {}).get('edges', [])

        # Safely handle empty data
        if not nodes:
            print("Warning: No nodes found to export.")
            nodes_df = pd.DataFrame()
        else:
            nodes_df = pd.DataFrame([n.get('data', {}) for n in nodes])

        if not edges:
             print("Warning: No edges found to export.")
             edges_df = pd.DataFrame()
        else:
            edges_df = pd.DataFrame([e.get('data', {}) for e in edges])

        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            nodes_df.to_excel(writer, sheet_name='Nodes', index=False)
            edges_df.to_excel(writer, sheet_name='Edges', index=False)

        print("Conversion Successful.")

    except Exception as e:
        print(f"Conversion Failed: {e}")

# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# MAIN EXECUTION BLOCK
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
if __name__ == "__main__":

    # <<< USER SELECTIONS (EDIT THIS SECTION) >>>
    # 1. NODE ANALYSIS
    RUN_NODE_ANALYSIS = False
    TARGET_NODE = "Keratinocytes"

    # 2. EDGE ANALYSIS
    RUN_EDGE_ANALYSIS = False
    EDGE_NODE_A = "Keratinocytes"
    EDGE_NODE_B = "T-Lymphocytes"

    # 3. GLOBAL TOP ARTICLES
    RUN_TOP_ARTICLES = False
    TOP_N_COUNT = 15

    # 4. EXCEL EXPORT
    RUN_EXCEL_EXPORT = True
    # Options: 'final_network', 'full_network', 'consensus_lcc'
    EXPORT_TARGET = 'final_network'

    # <<< EXECUTION LOGIC >>>
    print(f"Loaded Config for Project: {config.FILE_PREFIX}")

    if RUN_NODE_ANALYSIS:
        analyze_node_relevancy(TARGET_NODE)

    if RUN_EDGE_ANALYSIS:
        analyze_edge_relevancy(EDGE_NODE_A, EDGE_NODE_B)

    if RUN_TOP_ARTICLES:
        get_top_network_articles(limit=TOP_N_COUNT)

    if RUN_EXCEL_EXPORT:
        convert_network_json_to_excel(target_file_key=EXPORT_TARGET)