# -*- coding: utf-8 -*-
"""MeSH_Data_Processor_Git.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1REPxU3y6ag5Cl9kiJyEL4MkY2qrJBRzl
"""

# -*- coding: utf-8 -*-
"""
mesh_data_processor.py

A utility script to process raw MeSH data files (MARC and ASCII) from the NLM.
It performs three main tasks:
1. Extracts all unique MeSH Terms and UIs from the MARC binary file.
2. Generates a 'mesh_stop_words.py' file by filtering terms based on Tree Numbers in the ASCII file.
3. Tags the extracted CSV with a boolean flag indicating if a term is a stop word.

Usage:
    Ensure config.py UPDATE_MESH_SUPPORT_FILES = TRUE in order to update stop words.
    Ensure config.py is set up with 'mesh_marc' and 'mesh_ascii' with correct paths.
    Run: python mesh_data_processor.py
"""

import os
import sys
import traceback
from collections import defaultdict
import pandas as pd
from pymarc import MARCReader

# <<< Configuration Import >>>
# Dynamically find the project root to import config.py
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

try:
    import config
except ImportError:
    print("ERROR: Could not import 'config.py'. Ensure this script is in 'scripts/python/'")
    sys.exit(1)

# <<< Constants >>>
# Categories to KEEP (Anatomy, Diseases, Chemicals, Phenomena)
CATEGORIES_TO_KEEP = {'A', 'C', 'D', 'G'}

# Forced into the "Undetermined Categories" section.
CHECK_TAGS_TO_STOP = {'Male', 'Female'}

TOP_LEVEL_CATEGORY_NAMES = {
    'A': "Anatomy", 'B': "Organisms", 'C': "Diseases", 'D': "Chemicals and Drugs",
    'E': "Analytical, Diagnostic and Therapeutic Techniques, and Equipment",
    'F': "Psychiatry and Psychology", 'G': "Phenomena and Processes",
    'H': "Disciplines and Occupations",
    'I': "Anthropology, Education, Sociology, and Social Phenomena",
    'J': "Technology, Industry, Agriculture", 'K': "Humanities",
    'L': "Information Science", 'M': "Named Groups", 'N': "Health Care",
    'V': "Publication Characteristics", 'Z': "Geographicals"
}

# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# 1. MARC Extraction Logic
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

def extract_unique_mesh_terms_from_marc(marc_file_path, output_file_path):
    """
    Extracts unique DescriptorName (150a) and DescriptorUI (001) from MeSH MARC binary.
    """
    print(f"\n<<< Starting MARC Extraction >>>")
    print(f"Reading from: {marc_file_path}")

    if not os.path.exists(marc_file_path):
        print(f"ERROR: MARC file not found at {marc_file_path}")
        return False

    try:
        mesh_terms = []
        with open(marc_file_path, 'rb') as fh:
            reader = MARCReader(fh)
            count = 0
            for record in reader:
                count += 1
                if count % 5000 == 0:
                    print(f"  Scanned {count} records...", end='\r')

                # Field 150 = Heading, Field 001 = UI
                fields_150 = record.get_fields('150')
                fields_001 = record.get_fields('001')

                if fields_150 and fields_001:
                    # Extract Subfield 'a' from 150
                    subfields = fields_150[0].get_subfields('a')
                    descriptor_name = subfields[0] if subfields else None
                    descriptor_ui = fields_001[0].data

                    if descriptor_name and descriptor_ui:
                        mesh_terms.append([descriptor_name, descriptor_ui])

        print(f"  Scanned {count} records. Processing DataFrame...")

        df = pd.DataFrame(mesh_terms, columns=["DescriptorName", "DescriptorUI"])
        # Remove duplicates
        initial_len = len(df)
        df.drop_duplicates(subset=['DescriptorName'], keep='first', inplace=True)

        # Save
        df.to_csv(output_file_path, index=False)
        print(f"Successfully extracted {len(df)} unique terms (from {initial_len} raw) to '{output_file_path}'")
        return True

    except Exception as e:
        print(f"An unexpected error occurred during MARC extraction: {e}")
        traceback.print_exc()
        return False


# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# 2. Stop Word Generation Logic (ASCII)
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

def generate_stopwords_file(mesh_ascii_path, output_py_path):
    """
    Parses MeSH ASCII to find terms outside the kept categories and writes a Python module.
    """
    print(f"\n<<< Starting Stop Word Generation >>>")
    print(f"Reading from: {mesh_ascii_path}")

    if not os.path.exists(mesh_ascii_path):
        print(f"ERROR: MeSH ASCII file not found at {mesh_ascii_path}")
        return None

    mesh_term_to_categories = defaultdict(set)
    all_mh_found = set()
    current_mh = None

    try:
        with open(mesh_ascii_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line == "*NEWRECORD":
                    current_mh = None
                    continue

                if line.startswith("MH = "):
                    current_mh = line[len("MH = "):].strip()
                    all_mh_found.add(current_mh)

                elif line.startswith("MN = ") and current_mh:
                    tree_number = line[len("MN = "):].strip()
                    if tree_number and tree_number[0].isalpha():
                        top_cat = tree_number[0].upper()
                        mesh_term_to_categories[current_mh].add(top_cat)

    except Exception as e:
        print(f"Error parsing ASCII file: {e}")
        return None

    # Filter for Stop Words
    stop_words_grouped = defaultdict(set)
    terms_missing_tns = sorted(list(all_mh_found - set(mesh_term_to_categories.keys())))

    # Create the "Undetermined/Missing" set, starting with actual missing terms
    final_orphans_set = set(terms_missing_tns)

    for mh, cats in mesh_term_to_categories.items():
        # Special Case: Force 'Male'/'Female' into the orphan/missing list
        if mh in CHECK_TAGS_TO_STOP:
            final_orphans_set.add(mh)
            continue

        # Normal Logic: If ANY category is kept, the term is kept.
        if not any(c in CATEGORIES_TO_KEEP for c in cats):
            # Term is a stop word. Categorize by primary excluded category.
            excluded_cats = sorted(list(cats))
            primary_cat = excluded_cats[0] if excluded_cats else 'Unknown'
            stop_words_grouped[primary_cat].add(mh)

    # Ensure Check Tags are in the orphan set even if they weren't in the ASCII file for some reason
    for tag in CHECK_TAGS_TO_STOP:
        if tag in all_mh_found:
             final_orphans_set.add(tag)

    # Write the .py file
    write_stopwords_to_python(output_py_path, stop_words_grouped, sorted(list(final_orphans_set)))

    # Collect all stop words for the CSV tagging step
    all_stop_words = final_orphans_set.copy()
    for s in stop_words_grouped.values():
        all_stop_words.update(s)

    return all_stop_words

def write_stopwords_to_python(output_path, grouped_stopwords, missing_tns):
    """
    Writes the Python file with exact formatting from mesh_stop_words.py.
    """
    # 1. Reconstruct the exact list string: "['A - Anatomy', 'C - Diseases' ...]"
    cats_display_list = []
    for cat in sorted(list(CATEGORIES_TO_KEEP)):
        name = TOP_LEVEL_CATEGORY_NAMES.get(cat, "Unknown")
        cats_display_list.append(f"'{cat} - {name}'")
    cats_formatted_str = "[" + ", ".join(cats_display_list) + "]"

    try:
        with open(output_path, "w", encoding="utf-8") as f:
            # File Header - Exact wording
            f.write("# -*- coding: utf-8 -*-\n")
            f.write("# Combined MeSH Stop Word List\n")
            f.write("# Generated by parsing a MeSH ASCII descriptor file.\n\n")

            f.write("Mesh_stop_words = [\n")
            f.write("    # This list contains MeSH terms that generally do NOT fall into the primary\n")
            f.write(f"    # MeSH categories: {cats_formatted_str}.\n")
            f.write("    # It includes:\n")
            f.write("    #   1. Terms explicitly categorized outside of the 'kept' categories.\n")
            f.write("    #   2. Terms for which categories could not be determined (e.g., missing tree numbers in source).\n\n")

            f.write("    # --- Terms Categorized Outside A, C, D, G ---\n")

            # 2. Write Categorized Groups (Sorted by Category Letter)
            sorted_cats = sorted(grouped_stopwords.keys())
            for cat in sorted_cats:
                terms = sorted(list(grouped_stopwords[cat]))
                cat_desc = TOP_LEVEL_CATEGORY_NAMES.get(cat, "Unknown Category")

                f.write("\n")
                f.write("    #---------------------------------------------------------------------------\n")
                f.write(f"    # MeSH Category (source of these stop words): {cat} - {cat_desc}\n")
                f.write("    #---------------------------------------------------------------------------\n")

                for term in terms:
                    clean_term = term.replace('\\', '\\\\').replace('"', '\\"')
                    f.write(f'    "{clean_term}",\n')

            # 3. Write Undetermined / Missing Tree Numbers (Includes Male/Female)
            if missing_tns:
                f.write("\n")
                f.write("    #---------------------------------------------------------------------------\n")
                f.write("    # Terms With Undetermined Categories (from MH, no valid Tree Numbers found by script)\n")
                f.write("    # These are included as stop words as their category could not be confirmed to be A,C,D,G.\n")
                f.write("    #---------------------------------------------------------------------------\n")
                for term in missing_tns:
                    clean_term = term.replace('\\', '\\\\').replace('"', '\\"')
                    f.write(f'    "{clean_term}",\n')

            f.write("]\n")
        print(f"Successfully generated stop words file: {output_path}")

    except Exception as e:
        print(f"Error writing Python stop word file: {e}")


# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# 3. CSV Tagging Logic
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

def tag_csv_with_stopwords(csv_path, stop_words_set):
    """
    Adds a 'MeSH_stop_term' boolean column to the CSV.
    Uses vectorization for speed (vs the notebook's apply/lambda).
    """
    print(f"\n<<< Tagging CSV with Stop Words >>>")
    if not os.path.exists(csv_path):
        print("CSV file missing, cannot tag.")
        return

    try:
        df = pd.read_csv(csv_path)
        print(f"Loaded {len(df)} terms from {csv_path}")

        # Convert stop words set to lowercase for case-insensitive matching
        stop_words_lower = {s.lower() for s in stop_words_set}

        # Vectorized check
        df['MeSH_stop_term'] = df['DescriptorName'].str.lower().isin(stop_words_lower)

        stop_count = df['MeSH_stop_term'].sum()

        df.to_csv(csv_path, index=False)
        print(f"Tagged {stop_count} terms as stop words.")
        print(f"Updated CSV saved to: {csv_path}")

    except Exception as e:
        print(f"Error tagging CSV: {e}")
        traceback.print_exc()

# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# Main Execution
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

if __name__ == "__main__":
    # 1. Paths from Config
    marc_file = config.FILES.get('mesh_marc', os.path.join(config.RAW_DIR, "20250301_marc_full2025.bin"))
    ascii_file = config.FILES.get('mesh_ascii', os.path.join(config.RAW_DIR, "d2025.bin"))

    output_csv = config.FILES.get('mesh_terms_csv', os.path.join(config.PROCESSED_DIR, "mesh_terms.csv"))
    output_py = config.FILES.get('mesh_stopwords_py', os.path.join(config.PROJECT_ROOT, "scripts", "python", "mesh_stop_words.py"))

    force_update = getattr(config, 'UPDATE_MESH_SUPPORT_FILES', False)

    print(f"--- MeSH Data Processor ---")
    print(f"MARC Input:  {marc_file}")
    print(f"ASCII Input: {ascii_file}")
    print(f"CSV Output:  {output_csv}")
    print(f"PY Output:   {output_py}")
    print(f"Force Update: {force_update}")

    # 2. EXECUTION PHASE
    print("\n<<< Starting Processing >>>")

    # A. Extract Terms (MARC -> CSV)
    if not os.path.exists(output_csv) or force_update:
        extract_unique_mesh_terms_from_marc(marc_file, output_csv)
    else:
        print(f"Skipping MARC extraction (File exists and Force Update is OFF).")

    # B. Generate Stop Words (ASCII -> PY)
    generated_stop_set = None
    if not os.path.exists(output_py) or force_update:
        generated_stop_set = generate_stopwords_file(ascii_file, output_py)
    else:
        print(f"Skipping Stop Word Generation (File exists and Force Update is OFF).")

    # C. Tag CSV
    if generated_stop_set and os.path.exists(output_csv):
        tag_csv_with_stopwords(output_csv, generated_stop_set)
    else:
        if not generated_stop_set:
            print("\nSkipping CSV tagging (Stop words were not regenerated).")
        else:
            print(f"\nSkipping tagging: CSV file {output_csv} not found.")

    print("\nProcessing Complete.")